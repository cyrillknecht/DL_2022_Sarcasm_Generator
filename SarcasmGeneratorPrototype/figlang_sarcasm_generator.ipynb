{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "W5hISz6hWxOx"
   },
   "source": [
    "# Generating Sarcastic Tweets\n",
    "This notebook generates sarcastic tweets using the GPT-2 language model that has been fine tuned with sarcastic tweets from the FigLang dataset.\n",
    "It provides a standard finetuning procedure and a novel architecture that leverages generated Synthetic Data to enhance the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setup when using Google Colab."
   ],
   "metadata": {
    "id": "-WZvOv1rbjuS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5Gd82DXCWyTI",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670257639744,
     "user_tz": -60,
     "elapsed": 2285,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     }
    },
    "outputId": "9bf78efb-8c1d-467b-ce9f-18449770ee62"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# First mount your drive\n",
    "drive_path = Path('/content/drive')\n",
    "drive.mount(str(drive_path))\n",
    "\n",
    "# Set path to the project folder\n",
    "PROJECT_PATH = \"MyDrive/deep_learning/SarcasmGenerator\"\n",
    "path = drive_path / PROJECT_PATH\n",
    "\n",
    "# Possibly append to PATH\n",
    "if path not in sys.path:\n",
    "    sys.path.append(str(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16649,
     "status": "ok",
     "timestamp": 1670257656389,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     },
     "user_tz": -60
    },
    "id": "jHhe_tJkYMjW",
    "outputId": "cbb15a77-1936-4dc0-e197-0fad5cc284ee"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gpt-2-simple in /usr/local/lib/python3.8/dist-packages (0.8.1)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2022.6.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (1.21.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (4.64.1)\n",
      "Requirement already satisfied: toposort in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (1.7)\n",
      "Requirement already satisfied: tensorflow>=2.5.1 in /usr/local/lib/python3.8/dist-packages (from gpt-2-simple) (2.9.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.50.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.1.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (4.1.1)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.12)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.3.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (14.0.6)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.14.1)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.9.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (57.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (0.28.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.9.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.1.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.1.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (21.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (1.3.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (2.9.1)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.5.1->gpt-2-simple) (3.19.6)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.5.1->gpt-2-simple) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (2.14.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (5.2.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (4.13.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (3.10.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (0.4.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (2022.9.24)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gpt-2-simple) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow>=2.5.1->gpt-2-simple) (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow>=2.5.1->gpt-2-simple) (3.0.9)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.8/dist-packages (0.1.97)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gpt-2-simple\n",
    "!pip3 install transformers\n",
    "!pip3 install sentencepiece # fixes error while loading tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading Dataset from Google Drive."
   ],
   "metadata": {
    "id": "2OgnDkCFb5AE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4052,
     "status": "ok",
     "timestamp": 1670257660433,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     },
     "user_tz": -60
    },
    "id": "D4VdsO7CWxO1",
    "outputId": "2c71c3cd-ee88-4a50-fc80-611372f6393f"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0    @USER @USER @USER I don't get this .. obviousl...\n",
       "1    @USER @USER trying to protest about . Talking ...\n",
       "2    @USER @USER @USER He makes an insane about of ...\n",
       "3    @USER @USER Meanwhile Trump won't even release...\n",
       "4    @USER @USER Pretty Sure the Anti-Lincoln Crowd...\n",
       "Name: response, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "import pandas as pd\n",
    "\n",
    "FULL_DATASET_PATH = 'sarcasm_detection_shared_task_twitter_training.jsonl'\n",
    "\n",
    "train_data = pd.read_json(path/FULL_DATASET_PATH, lines=True)\n",
    "\n",
    "# Extract all sarcastic tweets from the dataset\n",
    "sarcastic_tweets = train_data[train_data[\"label\"]== \"SARCASM\"][\"response\"]\n",
    "sarcastic_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "To classify the generated output  a pretrained Sarcasm classifier trained on the same dataset was used."
   ],
   "metadata": {
    "id": "1sf7YHO5SJhr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "da6CzeBDS1go",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670257671717,
     "user_tz": -60,
     "elapsed": 11288,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     }
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "CLASSIFIER = \"mrm8488/t5-base-finetuned-sarcasm-twitter\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CLASSIFIER)\n",
    "classifier = AutoModelForSeq2SeqLM.from_pretrained(CLASSIFIER)\n",
    "\n",
    "def classify_tweets(tweet):\n",
    "  \"\"\" Classify a tweet using the pretrained classifier\n",
    "\n",
    "    Args:\n",
    "        tweet (str): Tweet or series of tweets to classify.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if tweet is classified as sarcastic.\n",
    "\n",
    "    \"\"\"\n",
    "  input_ids = tokenizer.encode(tweet + '</s>', return_tensors='pt')\n",
    "  output = classifier.generate(input_ids=input_ids, max_length=3)\n",
    "  dec = [tokenizer.decode(ids) for ids in output]\n",
    "  label = dec[0]\n",
    "  if label == '<pad> derison':\n",
    "    return True\n",
    "  \n",
    "  return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Testing the Classifier using a simulated conversation. Copied from Classifier documentation."
   ],
   "metadata": {
    "id": "uBUDURwsSpKI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2587,
     "status": "ok",
     "timestamp": 1670257674287,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     },
     "user_tz": -60
    },
    "id": "LNR0lI1vYXV8",
    "outputId": "85f7590e-4376-4bb6-fec6-7269d686d89d"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# For similarity with the training dataset we should replace users mentions in twits for @USER token and urls for URL token.\n",
    "\n",
    "twit1 = \"Trump just suspended the visa program that allowed me to move to the US to start @USER! Unfortunately, I won’t be able to vote in a few months but if you can, please vote him out, he's destroying what made America great in so many different ways!\"\n",
    "twit2 = \"@USER @USER @USER We have far more cases than any other country, so leaving remote workers in would be disastrous. Makes Trump sense.\"\n",
    "twit3 = \"My worry is that i wouldn’t be surprised if half the country actually agrees with this move...\"\n",
    "me = \"Trump doing so??? It must be a mistake... XDDD\"\n",
    "\n",
    "conversation = twit1 + twit2\n",
    "print(classify_tweets(conversation)) # Output: True\n",
    "\n",
    "conversation = twit1 + twit3\n",
    "print(classify_tweets(conversation)) # Output: False\n",
    "\n",
    "conversation = twit1 + me\n",
    "print(classify_tweets(conversation)) # Output: True\n",
    "\n",
    "# Example tweet obtained with normal fine-tuning\n",
    "print(classify_tweets(\"@USER @USER @USER Here's a funny one about celebrities #slaversmiley , because you're seeing it all\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Downloading pretrained GPT-2 model.\n",
    "(124M or 335M)"
   ],
   "metadata": {
    "id": "Elqi0vSocEIh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19266,
     "status": "ok",
     "timestamp": 1670257693546,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     },
     "user_tz": -60
    },
    "id": "UJRXeRL-WxO4",
    "outputId": "4097ed5a-0e58-42c1-b37e-032eb4aff1e6"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Fetching checkpoint: 1.05Mit [00:00, 326Mit/s]                                                      \n",
      "Fetching encoder.json: 1.05Mit [00:00, 5.09Mit/s]\n",
      "Fetching hparams.json: 1.05Mit [00:00, 695Mit/s]                                                    \n",
      "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:17, 28.5Mit/s]                                  \n",
      "Fetching model.ckpt.index: 1.05Mit [00:00, 385Mit/s]                                                \n",
      "Fetching model.ckpt.meta: 1.05Mit [00:00, 7.92Mit/s]\n",
      "Fetching vocab.bpe: 1.05Mit [00:00, 6.57Mit/s]\n"
     ]
    }
   ],
   "source": [
    "gpt2.download_gpt2(model_name=\"124M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initializing the model."
   ],
   "metadata": {
    "id": "RqBp9o0mchRM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "G5w24vLCWxO4",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670257694921,
     "user_tz": -60,
     "elapsed": 1386,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     }
    }
   },
   "outputs": [],
   "source": [
    "sess = gpt2.start_tf_sess()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Helper functions"
   ],
   "metadata": {
    "id": "NBrbGAwVzQAz"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(evaluation_samples=1):\n",
    "  \"\"\" Evaluate a model by generating tweets using the model and classifying them into sarcastic/non-sarcastic.\n",
    "\n",
    "    Args:\n",
    "        evaluation_samples (int):  # of runs of generating tweets.\n",
    "\n",
    "    Returns:\n",
    "      str: String describing the ratio of generated sarcastic to non-sarcastic tweets.\n",
    "\n",
    "    \"\"\"\n",
    "  sarc_tweet_counter = 0\n",
    "  tot_tweets_counter = 0\n",
    "\n",
    "  for i in range(evaluation_samples):\n",
    "    # Generate tweets with fine-tuned model\n",
    "    gen_tweets = gpt2.generate(sess,run_name='run',return_as_list=True)[0].split('\\n')\n",
    "\n",
    "    # Evaluate with pretrained classifier\n",
    "    for twt in gen_tweets:\n",
    "      sarc = classify_tweets(twt)\n",
    "      if sarc:\n",
    "        sarc_tweet_counter += 1\n",
    "\n",
    "    tot_tweets_counter += len(gen_tweets)\n",
    "\n",
    "  return f'{sarc_tweet_counter} of {tot_tweets_counter} generated tweets were classified as sarcastic.'"
   ],
   "metadata": {
    "id": "r1HZmXNgtw-h",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670257694922,
     "user_tz": -60,
     "elapsed": 4,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     }
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters for Standard Finetuning Procedure"
   ],
   "metadata": {
    "id": "TISkKFiX1OeJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL = '124M'\n",
    "STEPS = 20\n",
    "TRAIN_TWEETS_NR = 100\n",
    "TRAIN_SET_PATH = 'training_tweets.txt'"
   ],
   "metadata": {
    "id": "09EdKC3w1Ihn",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670257694922,
     "user_tz": -60,
     "elapsed": 4,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     }
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Standard Finetuning Procedure"
   ],
   "metadata": {
    "id": "cF0lDdI01ALl"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import time\n",
    "\n",
    "# Prepare training set\n",
    "training_tweets = sarcastic_tweets[:TRAIN_TWEETS_NR]\n",
    "training_tweets.to_csv(TRAIN_SET_PATH)\n",
    "\n",
    "\n",
    "# Finetune the model with sarcastic tweets\n",
    "start = time.time()\n",
    "\n",
    "gpt2.reset_session(sess)\n",
    "sess = gpt2.start_tf_sess()\n",
    "\n",
    "gpt2.finetune(sess,\n",
    "                dataset=TRAIN_SET_PATH,\n",
    "                model_name=MODEL,\n",
    "                steps=STEPS,\n",
    "                restore_from='fresh',\n",
    "                run_name='run',\n",
    "                print_every=10,\n",
    "                sample_every=200,\n",
    "                save_every=STEPS,\n",
    "                reuse=False\n",
    "                )\n",
    "  \n",
    "elapsed_time = time.time() - start\n",
    "print(f'Duration of Fine-tuning: {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7umCOp_wv9f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670257758007,
     "user_tz": -60,
     "elapsed": 63089,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     }
    },
    "outputId": "fd6aced6-1beb-4e59-ec96-630546b839e2"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3698.68it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset has 2920 tokens\n",
      "Training...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[10 | 24.94] loss=1.98 avg=1.98\n",
      "[20 | 46.61] loss=0.52 avg=1.25\n",
      "Saving checkpoint/run/model-20\n",
      "Duration of Fine-tuning: 00:01:02\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation of Standard Finetuning Procedure"
   ],
   "metadata": {
    "id": "0mYMyrwC1l1R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "print(evaluate_model(evaluation_samples=2))\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(f'Duration of Evaluation: {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cSc0n_vcxuS0",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670257809247,
     "user_tz": -60,
     "elapsed": 51244,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     }
    },
    "outputId": "e80cc45f-730a-499e-b9ea-6e1713e6faa3"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "49 of 114 generated tweets were classified as sarcastic.\n",
      "Duration of Evaluation: 00:00:51\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hyperparameters for finetuning with Self-Augmenting architecture"
   ],
   "metadata": {
    "id": "hIfbWruV0Oc1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL = '124M'\n",
    "EPOCHS = 2\n",
    "STEPS_PER_EPOCH = 10\n",
    "TRAIN_TWEETS = 100\n",
    "GENERATED_SAMPLES = 2\n",
    "TRAIN_SET_PATH = 'training_tweets.txt'"
   ],
   "metadata": {
    "id": "yrqJgofd0Nv7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670257809248,
     "user_tz": -60,
     "elapsed": 6,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     }
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "New Self-Augmenting Architecture Finetuning"
   ],
   "metadata": {
    "id": "trERXavRXOxX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 197160,
     "status": "ok",
     "timestamp": 1670258006404,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     },
     "user_tz": -60
    },
    "id": "--MSFoimWxO5",
    "outputId": "8f4de684-a094-4a19-b6a2-ac5af7a8fc73"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading checkpoint models/124M/model.ckpt\n",
      "Loading dataset...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4064.25it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset has 2920 tokens\n",
      "Training...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[10 | 24.62] loss=1.86 avg=1.86\n",
      "Saving checkpoint/run/model-10\n",
      "38 new sarcastic tweets were added to the training set in Epoch 0.\n",
      "Loading checkpoint checkpoint/run/model-10\n",
      "Loading dataset...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2786.91it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset has 3897 tokens\n",
      "Training...\n",
      "Saving checkpoint/run/model-10\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20 | 27.58] loss=1.12 avg=1.12\n",
      "Saving checkpoint/run/model-20\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/training/saver.py:1066: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (657 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "41 new sarcastic tweets were added to the training set in Epoch 1.\n",
      "Duration of Fine-tuning: 00:03:17\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Prepare training set\n",
    "training_tweets = sarcastic_tweets[:TRAIN_TWEETS]\n",
    "training_tweets.to_csv(TRAIN_SET_PATH)\n",
    "\n",
    "restore_from = 'fresh' # Initially use a new model\n",
    "\n",
    "# Augmented fine-tuning Loop\n",
    "for epoch in range(EPOCHS):\n",
    "  gpt2.reset_session(sess)\n",
    "  sess = gpt2.start_tf_sess()\n",
    "  gpt2.finetune(sess,\n",
    "                dataset=TRAIN_SET_PATH,\n",
    "                model_name=MODEL,\n",
    "                steps=STEPS_PER_EPOCH,\n",
    "                restore_from=restore_from,\n",
    "                run_name='run',\n",
    "                print_every=10,\n",
    "                sample_every=200,\n",
    "                save_every=STEPS_PER_EPOCH,\n",
    "                reuse=False\n",
    "                )\n",
    "  \n",
    "  restore_from = 'latest' # Reuse the same model while in training loop\n",
    "  \n",
    "  sarcastic_tweet_counter = 0\n",
    "\n",
    "  for sample in range(GENERATED_SAMPLES):\n",
    "    # Augmenting dataset with newly generated sarcastic tweets\n",
    "    generated_tweets = gpt2.generate(sess,run_name='run',return_as_list=True)[0].split('\\n')\n",
    "\n",
    "    # Selecting Sarcastic Tweets from generated tweets and adding them to the dataset\n",
    "    sarcasm_counter = 0\n",
    "    for tweet in generated_tweets:\n",
    "      sarcastic = classify_tweets(tweet)\n",
    "      if sarcastic:\n",
    "        with open(TRAIN_SET_PATH, \"a\") as train_file:\n",
    "          train_file.write(tweet)\n",
    "        sarcastic_tweet_counter += 1\n",
    "    \n",
    "  print(f\"{sarcastic_tweet_counter} new sarcastic tweets were added to the training set in Epoch {epoch}.\")\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(f'Duration of Fine-tuning: {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluation of Novel Finetuning Procedure"
   ],
   "metadata": {
    "id": "0JvEdbNYx5ak"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "start = time.time()\n",
    "\n",
    "print(evaluate_model(evaluation_samples=2))\n",
    "\n",
    "elapsed_time = time.time() - start\n",
    "print(f'Duration of Evaluation: {time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vsr6cf3dx20B",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670258051097,
     "user_tz": -60,
     "elapsed": 44708,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     }
    },
    "outputId": "8ed9d92b-c36e-4142-b6ba-09758df206e8"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "39 of 53 generated tweets were classified as sarcastic.\n",
      "Duration of evaluation: 00:00:44\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving a sample of generated Tweets to an output file for further use."
   ],
   "metadata": {
    "id": "7ha0If8EeL9s"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "CP3qdcp_WxO6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1670258068737,
     "user_tz": -60,
     "elapsed": 17645,
     "user": {
      "displayName": "Cyrill Knecht",
      "userId": "07128133178298927934"
     }
    }
   },
   "outputs": [],
   "source": [
    "OUTPUT_PATH = 'generated_tweets.txt'\n",
    "gpt2.generate(sess,run_name='run',destination_path=OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
